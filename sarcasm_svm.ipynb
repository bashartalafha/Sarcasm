{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd    \n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC, NuSVC, libsvm, SVR\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\", header=None)\n",
    "train = np.array(shuffle(train))\n",
    "\n",
    "test=pd.read_csv(\"test.csv\", header=None)\n",
    "test = np.array(shuffle(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1239 Test size: 315\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\",len(train), \"Test size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sum = []\n",
    "for i in range(len(train)):\n",
    "    s=0\n",
    "    for j in range(15,26): \n",
    "        s+=train[i][j]\n",
    "    train_sum.append(s)\n",
    "\n",
    "test_sum = []\n",
    "for i in range(len(test)):\n",
    "    s=0\n",
    "    for j in range(15,26):\n",
    "        s+=test[i][j]\n",
    "    test_sum.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [i[2] for i in train]\n",
    "test_text = [i[2] for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(train_arr, test_arr):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(train_arr)\n",
    "    train_mat = vectorizer.transform(train_arr)\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf.fit(train_mat)\n",
    "    train_tfmat = tfidf.transform(train_mat)\n",
    "\n",
    "    test_mat = vectorizer.transform(test_arr)\n",
    "    test_tfmat = tfidf.transform(test_mat)\n",
    "    return train_tfmat, test_tfmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two classes \n",
      "Score > 0 Yes\n",
      "Score < 0 No\n",
      "\n",
      "Training SVM model...\n",
      "accuracy: 0.8126984126984127\n"
     ]
    }
   ],
   "source": [
    "print(\"Two classes \\nScore > 0 Yes\\nScore < 0 No\")\n",
    "binary_train_labels = [\"Yes\" if i>0 else \"No\" for i in train_sum]\n",
    "binary_test_labels = [\"Yes\" if i>0 else \"No\" for i in test_sum]\n",
    "\n",
    "train_lbl = binary_train_labels\n",
    "test_lbl = binary_test_labels\n",
    "\n",
    "train_tfmat, test_tfmat = get_features(train_text, test_text)\n",
    "\n",
    "print(\"\\nTraining SVM model...\")\n",
    "lsvm=LinearSVC()\n",
    "lsvm.fit(train_tfmat,train_lbl)\n",
    "y_pred_lsvm=lsvm.predict(test_tfmat)\n",
    "print(\"accuracy:\", metrics.accuracy_score(test_lbl, y_pred_lsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three classes \n",
      "[4,11]-> Yes \n",
      "[-3,3]-> Conflict \n",
      "[-4,-11]-> No\n",
      "\n",
      "Training SVM model...\n",
      "accuracy: 0.6031746031746031\n"
     ]
    }
   ],
   "source": [
    "splitter = 3\n",
    "print(\"Three classes \\n[\"+str(splitter+1)+\",11]-> Yes \\n[-\"+str(splitter)+\",\"+str(splitter)+\"]-> Conflict \\n[-\"+str(splitter+1)+\",-11]-> No\")\n",
    "three_train_labels=[]\n",
    "for i in train_sum:\n",
    "    if i>splitter:\n",
    "        three_train_labels.append(\"Yes\")\n",
    "    elif i<=splitter and i>=-splitter:\n",
    "        three_train_labels.append(\"Conflict\")\n",
    "    elif i<-splitter:\n",
    "        three_train_labels.append(\"No\")\n",
    "        \n",
    "three_test_labels=[]\n",
    "for i in test_sum:\n",
    "    if i>splitter:\n",
    "        three_test_labels.append(\"Yes\")\n",
    "    elif i<=splitter and i>=-splitter:\n",
    "        three_test_labels.append(\"Conflict\")\n",
    "    elif i<-splitter:\n",
    "        three_test_labels.append(\"No\")\n",
    "        \n",
    "train_lbl = three_train_labels\n",
    "test_lbl = three_test_labels\n",
    "\n",
    "train_tfmat, test_tfmat = get_features(train_text, test_text)\n",
    "\n",
    "print(\"\\nTraining SVM model...\")\n",
    "lsvm=LinearSVC()\n",
    "lsvm.fit(train_tfmat,train_lbl)\n",
    "y_pred_lsvm=lsvm.predict(test_tfmat)\n",
    "print(\"accuracy:\", metrics.accuracy_score(test_lbl, y_pred_lsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
